
"""
```julia
dist_ll(dist::Collection, mu::Collection, sigma::Collection, tmin::Number, tmax::Number)
dist_ll(dist::Collection, analyses::Collection{<:Measurement}, tmin::Number, tmax::Number)
```
Return the log-likelihood of a set of mineral ages with means `mu` and
uncertianty `sigma` being drawn from a given source (i.e., crystallization / closure)
distribution `dist`, with terms to prevent runaway at low N.

### Examples
```julia
mu, sigma = collect(100:0.1:101), 0.01*ones(11)
ll = dist_ll(MeltsVolcanicZirconDistribution, mu, sigma, 100, 101)
```
"""
function dist_ll(dist::Collection, mu::Collection{<:Number}, sigma::Collection{<:Number}, tmin::Number, tmax::Number)
    tmax >= tmin || return NaN
    any(isnan, mu) && return NaN
    any(x->!(x>0), sigma) && return NaN
    nbins = length(dist) - 1
    Œît = abs(tmax-tmin)
    dt = Œît/nbins

    # Cycle through each datum in dataset
    ll = zero(float(eltype(dist)))
    @inbounds for j in eachindex(mu, sigma)
        Œº‚±º, œÉ‚±º = mu[j], sigma[j]

        # Find equivalent index position of Œº‚±º in the `dist` array
        ix = (Œº‚±º - tmin) / Œît * nbins + 1
        # If possible, prevent aliasing problems by interpolation
        if (œÉ‚±º < Œît / nbins) && 1 < ix < length(dist)
            # Interpolate corresponding distribution value
            f = floor(Int, ix)
            Œ¥ = ix - f
            likelihood = (dist[f+1]*Œ¥ + dist[f]*(1-Œ¥)) / Œît
        else
            # Otherwise, sum contributions from Gaussians at each point in distribution
            ùëñ = 1:length(dist)
            likelihood = zero(float(eltype(dist)))
            normconst = 1/(length(dist) * œÉ‚±º * sqrt(2 * pi))
            @turbo for i in eachindex(dist, ùëñ)
                distx = tmin + dt * (ùëñ[i] - 1) # time-position of distribution point
                # Likelihood curve follows a Gaussian PDF. Note: Œît cancels
                likelihood += dist[i] * normconst * exp(-(distx - Œº‚±º)^2 / (2 * œÉ‚±º * œÉ‚±º))
            end
        end
        ll += log(likelihood)
    end
    return ll
end
function dist_ll(dist::Collection, analyses::Collection{<:Measurement}, tmin::Number, tmax::Number)
    tmax >= tmin || return NaN
    any(isnan, analyses) && return NaN
    any(x->!(stdev(x) > 0), analyses) && return NaN
    nbins = length(dist) - 1
    Œît = abs(tmax - tmin)
    dt = Œît/nbins

    # Cycle through each datum in dataset
    ll = zero(float(eltype(dist)))
    @inbounds for j in eachindex(analyses)
        d‚±º = analyses[j]
        Œº‚±º, œÉ‚±º = value(d‚±º), stdev(d‚±º)

        # Find equivalent index position of Œº‚±º in the `dist` array
        ix = (Œº‚±º - tmin) / Œît * nbins + 1
        # If possible, prevent aliasing problems by interpolation
        if (œÉ‚±º < Œît / nbins) && 1 < ix < length(dist)
            # Interpolate corresponding distribution value
            f = floor(Int, ix)
            Œ¥ = ix - f
            likelihood = (dist[f+1]*Œ¥ + dist[f]*(1-Œ¥)) / Œît
        else
            # Otherwise, sum contributions from Gaussians at each point in distribution
            ùëñ = 1:length(dist)
            likelihood = zero(float(eltype(dist)))
            normconst = 1/(length(dist) * œÉ‚±º * sqrt(2 * pi))
            @inbounds @fastmath @simd ivdep for i in eachindex(dist, ùëñ)
                distx = tmin + dt * (ùëñ[i] - 1) # time-position of distribution point
                # Likelihood curve follows a Gaussian PDF. Note: Œît cancels
                likelihood += dist[i] * normconst * exp(-(distx - Œº‚±º)^2 / (2 * œÉ‚±º * œÉ‚±º))
            end
        end
        ll += log(likelihood)
    end
    return ll
end
function dist_ll(dist::Collection{T}, analyses::Collection{<:Distribution}, tmin::Number, tmax::Number) where {T<:AbstractFloat}
    tmax >= tmin || return T(NaN)

    tbinedges = range(tmin, tmax, length=length(dist)+1)
    @assert eachindex(tbinedges)[1:end-1] == eachindex(dist)
    dt = step(tbinedges)
    Œ£dist = sum(dist)

    # Cycle through each datum in dataset
    ll = zero(float(eltype(dist)))
    @inbounds for j in eachindex(analyses)
        d = analyses[j]
        Œº = mean(d)

        # Choose best approach (numerically speaking), given position of analysis
        if Œº < tmin
            # Diff log-CCDFs from young to old (left to right) 
            # if analysis is younger than tmin
            ll‚±º = typemin(T)
            lcdf_last = logccdf(d, tmin)
            for i in eachindex(dist)
                lcdf·µ¢ = logccdf(d, tbinedges[i+1])
                Œîlcdf = logsubexp(lcdf_last, lcdf·µ¢)
                ll‚±º = logaddexp(ll‚±º, Œîlcdf+log(dist[i]/(dt*Œ£dist)))

                # Prepare for next step
                lcdf_last = lcdf·µ¢
            end
            ll += ll‚±º
        elseif tmax < Œº
            # Diff log-CDFs from old to young (right to left) 
            # if analysis is older than tmax
            ll‚±º = typemin(T)
            lcdf_last = logcdf(d, tmax)
            for i in reverse(eachindex(dist))
                lcdf·µ¢ = logcdf(d, tbinedges[i])
                Œîlcdf = logsubexp(lcdf_last, lcdf·µ¢)
                ll‚±º = logaddexp(ll‚±º, Œîlcdf+log(dist[i]/(dt*Œ£dist)))

                # Prepare for next step
                lcdf_last = lcdf·µ¢
            end
            ll += ll‚±º
        else
            # Diff CCDFs from young to old (left to right)
            # if analysis is between tmin and tmax
            l‚±º = zero(T)
            cdf_last = ccdf(d, tmin)
            for i in eachindex(dist)
                cdf·µ¢ = ccdf(d, tbinedges[i+1])
                Œîcdf = cdf_last - cdf·µ¢
                l‚±º += Œîcdf * dist[i]/(dt*Œ£dist)

                # Prepare for next step
                cdf_last = cdf·µ¢
            end
            ll += log(l‚±º)
        end
    end
    return ll
end
function dist_ll(dist::Collection{T}, analyses::Collection{UPbAnalysis{T}}, tmin::Number, tmax::Number, tll::Number) where {T<:AbstractFloat}
    tmax >= tmin || return T(NaN)
    any(isnan, analyses) && return T(NaN)

    tbinedges = range(tmin, tmax, length=length(dist)+1)
    @assert eachindex(tbinedges)[1:end-1] == eachindex(dist)
    dt = step(tbinedges)
    Œ£dist = sum(dist)
    r75‚Çó‚Çó = ratio(tll, value(Œª235U))
    r68‚Çó‚Çó = ratio(tll, value(Œª238U))
    Œº‚Çó‚Çó = SVector(r75‚Çó‚Çó, r68‚Çó‚Çó)

    # Cycle through each datum in dataset
    ll = zero(float(eltype(dist)))
    @inbounds for j in eachindex(analyses)
        d = analyses[j]
        Œº‚±º = mean(d) - Œº‚Çó‚Çó
        Œ£‚±º = cov(d)

        # Rotate mean of analysis relative to line between tmin and tll
        r75_tmin = ratio(tmin, value(Œª235U))
        r68_tmin = ratio(tmin, value(Œª238U))
        R_tmin = RotMatrix(œÄ/2 - atan(r68_tmin-r68‚Çó‚Çó, r75_tmin-r75‚Çó‚Çó))
        Œº·µ£_tmin = R_tmin * Œº‚±º
        Œ£·µ£_tmin = R_tmin * Œ£‚±º * R_tmin'
        Œº‚ÇÅ_tmin, œÉ‚ÇÅ_tmin = first(Œº·µ£_tmin), sqrt(first(Œ£·µ£_tmin))

        # Rotate mean of analysis relative to line between tmax and tll
        r75_tmax = ratio(tmax, value(Œª235U)) 
        r68_tmax = ratio(tmax, value(Œª238U))
        R_tmax = RotMatrix(œÄ/2 - atan(r68_tmax-r68‚Çó‚Çó, r75_tmax-r75‚Çó‚Çó))
        Œº·µ£_tmax = R_tmax * Œº‚±º
        Œ£·µ£_tmax = R_tmax * Œ£‚±º * R_tmax'
        Œº‚ÇÅ_tmax, œÉ‚ÇÅ_tmax = first(Œº·µ£_tmax), sqrt(first(Œ£·µ£_tmax))

        # Choose best approach (numerically speaking), given position of analysis
        if Œº‚ÇÅ_tmin < 0 
            # Diff log-CCDFs from young to old (left to right) 
            # if analysis is left of youngest discordia array
            ll‚±º = typemin(T)
            lcdf_last = logccdf(Normal(Œº‚ÇÅ_tmin, œÉ‚ÇÅ_tmin), zero(T))
            for i in eachindex(dist)
                # Rotation matrix that would rotate discordia line between tll and tbinedges[i+1] to vertical
                r75·µ¢ = ratio(tbinedges[i+1], value(Œª235U))
                r68·µ¢ = ratio(tbinedges[i+1], value(Œª238U))
                R·µ¢ = RotMatrix(œÄ/2 - atan(r68·µ¢-r68‚Çó‚Çó, r75·µ¢-r75‚Çó‚Çó))

                # Rotate means and covariance matrix, with proposed time of Pb-loss at origin
                Œº·µ£ = R·µ¢ * Œº‚±º
                Œ£·µ£ = R·µ¢ * Œ£‚±º * R·µ¢'
                Œº‚ÇÅ, œÉ‚ÇÅ = first(Œº·µ£), sqrt(first(Œ£·µ£))
                lcdf·µ¢ = logccdf(Normal(Œº‚ÇÅ, œÉ‚ÇÅ), zero(T))
                Œîlcdf = logsubexp(lcdf_last, lcdf·µ¢)
                ll‚±º = logaddexp(ll‚±º, Œîlcdf+log(dist[i]/(dt*Œ£dist)))

                # Prepare for next step
                lcdf_last = lcdf·µ¢
            end
            ll += ll‚±º
        elseif Œº‚ÇÅ_tmax > 0
            # Diff log-CDFs from old to young (right to left) 
            # if analysis is right of oldest discordia array
            ll‚±º = typemin(T)
            lcdf_last = logcdf(Normal(Œº‚ÇÅ_tmax, œÉ‚ÇÅ_tmax), zero(T))
            for i in reverse(eachindex(dist))
                # Rotation matrix that would rotate discordia line between tll and tbinedges[i] to vertical
                r75·µ¢ = ratio(tbinedges[i], value(Œª235U))
                r68·µ¢ = ratio(tbinedges[i], value(Œª238U))
                R·µ¢ = RotMatrix(œÄ/2 - atan(r68·µ¢-r68‚Çó‚Çó, r75·µ¢-r75‚Çó‚Çó))

                # Rotate means and covariance matrix, with proposed time of Pb-loss at origin
                Œº·µ£ = R·µ¢ * Œº‚±º
                Œ£·µ£ = R·µ¢ * Œ£‚±º * R·µ¢'
                Œº‚ÇÅ, œÉ‚ÇÅ = first(Œº·µ£), sqrt(first(Œ£·µ£))
                lcdf·µ¢ = logcdf(Normal(Œº‚ÇÅ, œÉ‚ÇÅ), zero(T))
                Œîlcdf = logsubexp(lcdf_last, lcdf·µ¢)
                ll‚±º = logaddexp(ll‚±º, Œîlcdf+log(dist[i]/(dt*Œ£dist)))

                # Prepare for next step
                lcdf_last = lcdf·µ¢
            end
            ll += ll‚±º
        else
            # Diff CCDFs from young to old (left to right)
            # if analysis is between youngest and oldest discordia array
            l‚±º = zero(T)
            cdf_last = ccdf(Normal(Œº‚ÇÅ_tmin, œÉ‚ÇÅ_tmin), zero(T))
            for i in eachindex(dist)
                # Rotation matrix that would rotate discordia line between tll and tbinedges[i+1] to vertical
                r75·µ¢ = ratio(tbinedges[i+1], value(Œª235U))
                r68·µ¢ = ratio(tbinedges[i+1], value(Œª238U))
                R·µ¢ = RotMatrix(œÄ/2 - atan(r68·µ¢-r68‚Çó‚Çó, r75·µ¢-r75‚Çó‚Çó))

                # Rotate means and covariance matrix, with proposed time of Pb-loss at origin
                Œº·µ£ = R·µ¢ * Œº‚±º
                Œ£·µ£ = R·µ¢ * Œ£‚±º * R·µ¢'
                Œº‚ÇÅ, œÉ‚ÇÅ = first(Œº·µ£), sqrt(first(Œ£·µ£))
                cdf·µ¢ = ccdf(Normal(Œº‚ÇÅ, œÉ‚ÇÅ), zero(T))
                Œîcdf = cdf_last - cdf·µ¢
                l‚±º += Œîcdf * dist[i]/(dt*Œ£dist)

                # Prepare for next step
                cdf_last = cdf·µ¢
            end
            ll += log(l‚±º)
        end
    end
    return ll
end


# Encoding of additional prior assumptions about likely age given observed dispersion
function prior_ll(mu::Collection, sigma::Collection, tmin::Number, tmax::Number)
    i‚Çã = argmin(mu)
    mu‚Çã, sigma‚Çã = mu[i‚Çã], sigma[i‚Çã]
    i‚Çä = argmax(mu)
    mu‚Çä, sigma‚Çä = mu[i‚Çä], sigma[i‚Çä]
    # Calculate a weighted mean and examine our MSWD
    (wm, wsigma, mswd) = wmean(mu, sigma, corrected=true) 
    # Degrees of freedom
    f = length(mu) - 1
    return prior_ll(wm, wsigma, mswd, mu‚Çã, sigma‚Çã, mu‚Çä, sigma‚Çä, f, tmin, tmax)
end
function prior_ll(analyses::Collection{<:Union{Distribution, Measurement}}, tmin::Number, tmax::Number)
    a‚Çã = argmin(value, analyses)
    a‚Çä = argmax(value, analyses)
    # Calculate a weighted mean and examine our MSWD
    (wm, mswd) = wmean(analyses, corrected=true)
    # Degrees of freedom
    f = length(analyses) - 1
    return prior_ll(value(wm), stdev(wm), mswd, value(a‚Çã), stdev(a‚Çã), value(a‚Çä), stdev(a‚Çä), f, tmin, tmax)
end
function prior_ll(wm, wsigma, mswd, mu‚Çã, sigma‚Çã, mu‚Çä, sigma‚Çä, f, tmin, tmax)
    # Height of MSWD distribution relative to height at MSWD = 1
    # (see Wendt and Carl, 1991, Chemical Geology)
    Zf = exp((f/2-1)*log(mswd) - f/2*(mswd-1)) * (f > 0)
    Zf = max(min(Zf, 1.0), 0.0)
    # To avoid instability of the MCMC for small datasets (low N),
    # favor the weighted mean interpretation at high Zf (MSWD close to 1) and
    # the youngest-zircon interpretation at low Zf (MSWD far from one). The
    # specific penalty factors applied here were determined by training  
    # against synthetic datasets in Keller et al. 2018.
    # In other words, these are basically context-dependent prior distributions on tmax and tmin
    ll = -(2/log(2+f)) * (                                # Scaling factor that decreases with log number of data points (i.e., no penalty at high N)
        log((abs(tmin - wm)+wsigma)/wsigma)*Zf +          # Penalty for proposing tmin too far from the weighted mean at low MSWD (High Zf)
        log((abs(tmax - wm)+wsigma)/wsigma)*Zf +          # Penalty for proposing tmax too far from the weighted mean at low MSWD (High Zf)
        log((abs(tmin - mu‚Çã)+sigma‚Çã)/sigma‚Çã)*(1-Zf) +     # Penalty for proposing tmin too far from youngest zircon at high MSWD (low Zf)
        log((abs(tmax - mu‚Çä)+sigma‚Çä)/sigma‚Çä)*(1-Zf)       # Penalty for proposing tmax too far from oldest zircon at high MSWD (low Zf)
    )
    return ll
end


"""
```julia
metropolis_min(nsteps::Integer, dist::Collection, data::Collection{<:Measurement}; burnin::Integer=0, t0prior=Uniform(0,minimum(value.(age68.(analyses)))), lossprior=Uniform(0,100))
metropolis_min(nsteps::Integer, dist::Collection, mu::AbstractArray, sigma::AbstractArray; burnin::Integer=0)
metropolis_min(nsteps::Integer, dist::Collection, analyses::Collection{<:UPbAnalysis; burnin::Integer=0)
```
Run a Metropolis sampler to estimate the minimum of a finite-range source
distribution `dist` using samples drawn from that distribution -- e.g., estimate
zircon eruption ages from a distribution of zircon crystallization ages.

### Examples
```julia
tmindist = metropolis_min(2*10^5, MeltsVolcanicZirconDistribution, mu, sigma, burnin=10^5)

tmindist, t0dist = metropolis_min(2*10^5, HalfNormalDistribution, analyses, burnin=10^5)
```
"""
function metropolis_min(nsteps::Integer, dist::Collection{T}, mu::Collection, sigma::Collection; kwargs...) where {T}
    tmindist = arraylike(mu, T, nsteps)
    metropolis_min!(tmindist, dist, mu, sigma; kwargs...)
    return tmindist
end
function metropolis_min(nsteps::Integer, dist::Collection{T}, data::Collection{<:Union{Measurement, Distribution}}; kwargs...) where {T}
    tmindist = arraylike(data, T, nsteps)
    metropolis_min!(tmindist, dist, data; kwargs...)
    return tmindist
end
function metropolis_min(nsteps::Integer, dist::Collection{T}, analyses::Collection{UPbAnalysis{T}}; kwargs...) where {T}
    tmindist = arraylike(analyses, T, nsteps)
    t0dist = arraylike(analyses, T, nsteps)
    metropolis_min!(tmindist, t0dist, dist, analyses; kwargs...)
    return tmindist, t0dist
end


"""
```julia
metropolis_min!(tmindist::DenseArray, dist::Collection, mu::AbstractArray, sigma::AbstractArray; burnin::Integer=0)
metropolis_min!(tmindist::DenseArray, t0dist::DenseArray, dist::Collection, analyses::Collection{<:UPbAnalysis}; burnin::Integer=0) where {T}
```
In-place (non-allocating) version of `metropolis_min`, fills existing array `tmindist`.

Run a Metropolis sampler to estimate the minimum of a finite-range source
distribution `dist` using samples drawn from that distribution -- e.g., estimate
zircon eruption ages from a distribution of zircon crystallization ages.

### Examples
```julia
metropolis_min!(tmindist, 2*10^5, MeltsVolcanicZirconDistribution, mu, sigma, burnin=10^5)
```
"""
function metropolis_min!(tmindist::DenseArray{<:Number}, dist::Collection{<:Number}, mu::AbstractArray{<:Number}, sigma::AbstractArray{<:Number}; burnin::Integer=0)
    @assert eachindex(mu) == eachindex(sigma)

    # standard deviation of the proposal function is stepfactor * last step; this is tuned to optimize accetance probability at 50%
    stepfactor = 2.9

    # Initial step sigma for Gaussian proposal distributions
    youngest, oldest = minimum(mu), maximum(mu)
    Œît = (oldest - youngest) + sqrt(sigma[argmin(mu)]^2 + sigma[argmax(mu)]^2)
    tminstep = tmaxstep = Œît / length(mu)

    # Use oldest and youngest zircons for initial proposal
    tmin‚Çö = tmin = youngest - sigma[argmin(mu)]
    tmax‚Çö = tmax = oldest + sigma[argmax(mu)]

    # Log likelihood of initial proposal
    ll‚Çö = ll = dist_ll(dist, mu, sigma, tmin, tmax) + prior_ll(mu, sigma, tmin, tmax)

    # Burnin
    for i=1:burnin
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, mu, sigma, tmin‚Çö, tmax‚Çö) + prior_ll(mu, sigma, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
        end
    end
    # Step through each of the N steps in the Markov chain
    @inbounds for i in eachindex(tmindist)
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, mu, sigma, tmin‚Çö, tmax‚Çö) + prior_ll(mu, sigma, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
        end
        tmindist[i] = tmin
    end
    return tmindist
end
function metropolis_min!(tmindist::DenseArray{<:Number}, dist::Collection{<:Number}, data::AbstractArray{<:Union{Distribution, Measurement}}; burnin::Integer=0)

    # standard deviation of the proposal function is stepfactor * last step; this is tuned to optimize accetance probability at 50%
    stepfactor = 2.9

    # Initial step sigma for Gaussian proposal distributions
    youngest, oldest = argmin(value, data), argmax(value, data)
    Œît = (value(oldest)- value(youngest)) + sqrt(stdev(oldest)^2 + stdev(youngest)^2)
    tminstep = tmaxstep = Œît / length(data)

    # Use oldest and youngest data for initial proposal
    tmin‚Çö = tmin = value(youngest) - stdev(youngest)
    tmax‚Çö = tmax = value(oldest) + stdev(oldest)

    # Log likelihood of initial proposal
    ll‚Çö = ll = dist_ll(dist, data, tmin, tmax) + prior_ll(data, tmin, tmax)

    # Burnin
    for i=1:burnin
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, data, tmin‚Çö, tmax‚Çö) + prior_ll(data, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
        end
    end
    # Step through each of the N steps in the Markov chain
    @inbounds for i in eachindex(tmindist)
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, data, tmin‚Çö, tmax‚Çö) + prior_ll(data, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
        end
        tmindist[i] = tmin
    end
    return tmindist
end
function metropolis_min!(tmindist::DenseArray{T}, tlldist::DenseArray{T}, dist::Collection{T}, analyses::Collection{UPbAnalysis{T}}; burnin::Integer=0, tllprior=Uniform(0,minimum(value.(age68.(analyses)))), lossprior=Uniform(0,100), method=:projection) where {T<:AbstractFloat}
    @assert eachindex(tmindist) == eachindex(tlldist)
    @assert (method === :projection || method === :bivariate) "Allowed methods are `:projection` or `:bivariate`"

    # standard deviation of the proposal function is stepfactor * last step; this is tuned to optimize accetance probability at 50%
    stepfactor = 2.9

    # Process input analyses
    ellipses = Ellipse.(analyses)
    ages68 = log.(one(T) .+ (ellipses .|> e->e.y‚ÇÄ))./value(Œª238U)
    ages = @. upperintercept(zero(T), ellipses)

    # Initial step sigma for Gaussian proposal distributions
    youngest, oldest = minimum(ages), maximum(ages)
    Œît = (oldest.val - youngest.val) + sqrt(+ oldest.err^2 + youngest.err^2)
    tminstep = tmaxstep = Œît / length(analyses)
    tllstep = youngest.val/50

    # Use oldest and youngest zircons for initial proposal
    tmin‚Çö = tmin = value(youngest)
    tmax‚Çö = tmax = value(oldest)
    tll‚Çö = tll = zero(T)

    # Ensure priors for time and amount of lead loss are appropriately truncated
    tllprior = truncated(tllprior, 0, minimum(ages68))
    lossprior = truncated(lossprior, 0, 100) # percent

    # Log likelihood of initial proposal
    ll = logpdf(tllprior, tll)
    if method === :projection
        ll += dist_ll(dist, ages, tmin, tmax) 
    elseif method === :bivariate
        ll += dist_ll(dist, analyses, tmin, tmax, tll) 
    end
    ll += prior_ll(ages, tmin, tmax)
    for i in eachindex(ages, ages68)
        loss = 100*max(one(T) - (ages68[i] - tll) / (value(ages[i]) - tll), zero(T))
        ll += logpdf(lossprior, loss)
    end
    ll‚Çö = ll

    # Burnin
    for i = 1:burnin
        tmin‚Çö, tmax‚Çö, tll‚Çö = tmin, tmax, tll
        # Adjust upper or lower bounds, or Pb-loss time
        r = rand()
        if r < 0.35
            tmin‚Çö += tminstep * randn()
        elseif r < 0.70
            tmax‚Çö += tmaxstep * randn()
        else
            tll‚Çö += tllstep * randn()
        end
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        @. ages = upperintercept(tll‚Çö, ellipses)
        ll‚Çö = logpdf(tllprior, tll‚Çö)
        if method === :projection
            ll‚Çö += dist_ll(dist, ages, tmin‚Çö, tmax‚Çö)
        elseif method === :bivariate
            ll‚Çö += dist_ll(dist, analyses, tmin‚Çö, tmax‚Çö, tll‚Çö)
        end
        ll‚Çö += prior_ll(ages, tmin‚Çö, tmax‚Çö)
        for i in eachindex(ages, ages68)
            loss = 100*max(one(T) - (ages68[i] - tll‚Çö) / (value(ages[i]) - tll‚Çö), zero(T))
            ll‚Çö += logpdf(lossprior, loss)
        end
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö - ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö - tmin) * stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö - tmax) * stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
            tll = tll‚Çö
        end
    end
    # Step through each of the N steps in the Markov chain
    @inbounds for i in eachindex(tmindist, tlldist)
        tmin‚Çö, tmax‚Çö, tll‚Çö = tmin, tmax, tll
        # Adjust upper or lower bounds, or Pb-loss time
        r = rand()
        if r < 0.35
            tmin‚Çö += tminstep * randn()
        elseif r < 0.70
            tmax‚Çö += tmaxstep * randn()
        else
            tll‚Çö += tllstep * randn()
        end
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        @. ages = upperintercept(tll‚Çö, ellipses)
        ll‚Çö = logpdf(tllprior, tll‚Çö)
        if method === :projection
            ll‚Çö += dist_ll(dist, ages, tmin‚Çö, tmax‚Çö)
        elseif method === :bivariate
            ll‚Çö += dist_ll(dist, analyses, tmin‚Çö, tmax‚Çö, tll‚Çö)
        end
        ll‚Çö += prior_ll(ages, tmin‚Çö, tmax‚Çö)
        for i in eachindex(ages, ages68)
            loss = 100*max(one(T) - (ages68[i] - tll‚Çö) / (value(ages[i]) - tll‚Çö), zero(T))
            ll‚Çö += logpdf(lossprior, loss)
        end
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö - ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö - tmin) * stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö - tmax) * stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
            tll = tll‚Çö
        end
        tmindist[i] = tmin
        tlldist[i] = tll
    end
    return tmindist, tlldist
end

"""
```julia
metropolis_minmax(nsteps::Integer, dist::Collection, data::Collection{<:Measurement}; burnin::Integer=0)
metropolis_minmax(nsteps::Integer, dist::AbstractArray, data::AbstractArray, uncert::AbstractArray; burnin::Integer=0)
```
Run a Metropolis sampler to estimate the extrema of a finite-range source
distribution `dist` using samples drawn from that distribution -- e.g.,
estimate zircon saturation and eruption ages from a distribution of zircon
crystallization ages.

### Examples
```julia
tmindist, tmaxdist, lldist, acceptancedist = metropolis_minmax(2*10^5, MeltsVolcanicZirconDistribution, mu, sigma, burnin=10^5)
```
"""
function metropolis_minmax(nsteps::Integer, dist::Collection{T}, mu::AbstractArray, sigma::AbstractArray; kwargs...) where {T}
    acceptancedist = falses(nsteps)
    lldist = arraylike(mu, T, nsteps)
    tmaxdist = arraylike(mu, T, nsteps)
    tmindist = arraylike(mu, T, nsteps)
    metropolis_minmax!(tmindist, tmaxdist, lldist, acceptancedist, dist, mu, sigma; kwargs...)
    return tmindist, tmaxdist, lldist, acceptancedist
end
function metropolis_minmax(nsteps::Integer, dist::Collection{T}, data::Collection{<:Union{Measurement, Distribution}}; kwargs...) where {T}
    acceptancedist = falses(nsteps)
    lldist = arraylike(data, T, nsteps)
    tmaxdist = arraylike(data, T, nsteps)
    tmindist = arraylike(data, T, nsteps)
    metropolis_minmax!(tmindist, tmaxdist, lldist, acceptancedist, dist, data; kwargs...)
    return tmindist, tmaxdist, lldist, acceptancedist
end
function metropolis_minmax(nsteps::Integer, dist::Collection{T}, analyses::Collection{<:UPbAnalysis{T}}; kwargs...) where {T<:AbstractFloat}
    acceptancedist = falses(nsteps)
    lldist = arraylike(analyses, T, nsteps)
    t0dist = arraylike(analyses, T, nsteps)
    tmaxdist = arraylike(analyses, T, nsteps)
    tmindist = arraylike(analyses, T, nsteps)
    metropolis_minmax!(tmindist, tmaxdist, t0dist, lldist, acceptancedist, dist, analyses; kwargs...)
    return tmindist, tmaxdist, t0dist, lldist, acceptancedist
end

"""
```julia
metropolis_minmax!(tmindist, tmaxdist, lldist, acceptancedist, nsteps::Integer, dist::AbstractArray, data::AbstractArray, uncert::AbstractArray; burnin::Integer=0)
metropolis_minmax!(tmindist, tmaxdist, t0dist, lldist, acceptancedist, nsteps::Integer, dist::Collection, analyses::Collection{<:UPbAnalysis}; burnin::Integer=0)
```
In-place (non-allocating) version of `metropolis_minmax`, filling existing arrays

Run a Metropolis sampler to estimate the extrema of a finite-range source
distribution `dist` using samples drawn from that distribution -- e.g.,
estimate zircon saturation and eruption ages from a distribution of zircon
crystallization ages.

### Examples
```julia
metropolis_minmax!(tmindist, tmaxdist, lldist, acceptancedist, 2*10^5, MeltsVolcanicZirconDistribution, mu, sigma, burnin=10^5)
```
"""
function metropolis_minmax!(tmindist::DenseArray{<:Number}, tmaxdist::DenseArray{<:Number}, lldist::DenseArray{<:Number}, acceptancedist::BitVector, dist::Collection{<:Number}, mu::AbstractArray{<:Number}, sigma::AbstractArray{<:Number}; burnin::Integer=0)
    @assert eachindex(tmindist) == eachindex(tmaxdist) == eachindex(lldist) == eachindex(acceptancedist) 
    @assert eachindex(mu) == eachindex(sigma)

    # standard deviation of the proposal function is stepfactor * last step; this is tuned to optimize accetance probability at 50%
    stepfactor = 2.9

    # Initial step sigma for Gaussian proposal distributions
    youngest, oldest = minimum(mu), maximum(mu)
    Œît = (oldest - youngest) + sqrt(sigma[argmin(mu)]^2 + sigma[argmax(mu)]^2)
    tminstep = tmaxstep = Œît / length(mu)

    # Use oldest and youngest zircons for initial proposal
    tmin‚Çö = tmin = youngest - sigma[argmin(mu)]
    tmax‚Çö = tmax = oldest + sigma[argmax(mu)]

    # Log likelihood of initial proposal
    ll‚Çö = ll = dist_ll(dist, mu, sigma, tmin, tmax) + prior_ll(mu, sigma, tmin, tmax)

    # Burnin
    for i=1:burnin
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, mu, sigma, tmin‚Çö, tmax‚Çö) + prior_ll(mu, sigma, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
        end
    end
    # Step through each of the N steps in the Markov chain
    @inbounds for i in eachindex(tmindist, tmaxdist, lldist, acceptancedist)
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, mu, sigma, tmin‚Çö, tmax‚Çö) + prior_ll(mu, sigma, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
            acceptancedist[i]=true
        end
        tmindist[i] = tmin
        tmaxdist[i] = tmax
        lldist[i] = ll
    end
    return tmindist, tmaxdist, lldist, acceptancedist
end
function metropolis_minmax!(tmindist::DenseArray{<:Number}, tmaxdist::DenseArray{<:Number}, lldist::DenseArray{<:Number}, acceptancedist::BitVector, dist::Collection{<:Number}, data::AbstractArray{<:Union{Distribution, Measurement}}; burnin::Integer=0)
    @assert eachindex(tmindist) == eachindex(tmaxdist) == eachindex(lldist) == eachindex(acceptancedist) 

    # standard deviation of the proposal function is stepfactor * last step; this is tuned to optimize accetance probability at 50%
    stepfactor = 2.9

    # Initial step sigma for Gaussian proposal distributions
    youngest, oldest = argmin(value, data), argmax(value, data)
    Œît = (value(oldest)- value(youngest)) + sqrt(stdev(oldest)^2 + stdev(youngest)^2)
    tminstep = tmaxstep = Œît / length(data)

    # Use oldest and youngest data for initial proposal
    tmin‚Çö = tmin = value(youngest) - stdev(youngest)
    tmax‚Çö = tmax = value(oldest) + stdev(oldest)

    # Log likelihood of initial proposal
    ll‚Çö = ll = dist_ll(dist, data, tmin, tmax) + prior_ll(data, tmin, tmax)

    # Burnin
    for i=1:burnin
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, data, tmin‚Çö, tmax‚Çö) + prior_ll(data, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
        end
    end
    # Step through each of the N steps in the Markov chain
    @inbounds for i in eachindex(tmindist, tmaxdist, lldist, acceptancedist)
        # Adjust upper or lower bounds
        tmin‚Çö, tmax‚Çö = tmin, tmax
        r = rand()
        (r < 0.5) && (tmax‚Çö += tminstep*randn())
        (r > 0.5) && (tmin‚Çö += tmaxstep*randn())
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        ll‚Çö = dist_ll(dist, data, tmin‚Çö, tmax‚Çö) + prior_ll(data, tmin‚Çö, tmax‚Çö)
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö-ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö-tmin)*stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö-tmax)*stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
            acceptancedist[i] = true
        end
        tmindist[i] = tmin
        tmaxdist[i] = tmax
        lldist[i] = ll
    end
    return tmindist, tmaxdist, lldist, acceptancedist
end
function metropolis_minmax!(tmindist::DenseArray{T}, tmaxdist::DenseArray{T}, tlldist::DenseArray{T}, lldist::DenseArray{T}, acceptancedist::BitVector, dist::Collection{T}, analyses::Collection{UPbAnalysis{T}}; burnin::Integer=0, tllprior=Uniform(0,minimum(value.(age68.(analyses)))), lossprior=Uniform(0,100), method=:projection) where {T <: AbstractFloat}
    @assert eachindex(tmindist) == eachindex(tmaxdist) == eachindex(tlldist) == eachindex(lldist) == eachindex(acceptancedist)
    @assert (method === :projection || method === :bivariate) "Allowed methods are `:projection` or `:bivariate`"

    # standard deviation of the proposal function is stepfactor * last step; this is tuned to optimize accetance probability at 50%
    stepfactor = 2.9

    # Process input analyses
    ellipses = Ellipse.(analyses)
    ages68 = log.(one(T) .+ (ellipses .|> e->e.y‚ÇÄ))./value(Œª238U)
    ages = @. upperintercept(zero(T), ellipses)

    # Initial step sigma for Gaussian proposal distributions
    youngest, oldest = minimum(ages), maximum(ages)
    Œît = (oldest.val - youngest.val) + sqrt(+ oldest.err^2 + youngest.err^2)
    tminstep = tmaxstep = Œît / length(analyses)
    tllstep = youngest.val/50

    # Use oldest and youngest upper intercept ages for initial proposal
    tmin‚Çö = tmin = value(youngest)
    tmax‚Çö = tmax = value(oldest)
    tll‚Çö = tll = zero(T)

    # Ensure priors for time and amount of lead loss are appropriately truncated
    tllprior = truncated(tllprior, 0, minimum(ages68))
    lossprior = truncated(lossprior, 0, 100) # percent

    # Log likelihood of initial proposal
    ll = logpdf(tllprior, tll)
    if method === :projection
        ll += dist_ll(dist, ages, tmin, tmax) 
    elseif method === :bivariate
        ll += dist_ll(dist, analyses, tmin, tmax, tll) 
    end
    ll += prior_ll(ages, tmin, tmax)
    for i in eachindex(ages, ages68)
        loss = 100*max(one(T) - (ages68[i] - tll) / (value(ages[i]) - tll), zero(T))
        ll += logpdf(lossprior, loss)
    end
    ll‚Çö = ll

    # Burnin
    for i = 1:burnin
        tmin‚Çö, tmax‚Çö, tll‚Çö = tmin, tmax, tll
        # Adjust upper or lower bounds, or Pb-loss time
        r = rand()
        if r < 0.35
            tmin‚Çö += tminstep * randn()
        elseif r < 0.70
            tmax‚Çö += tmaxstep * randn()
        else
            tll‚Çö += tllstep * randn()
        end
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        @. ages = upperintercept(tll‚Çö, ellipses)
        ll‚Çö = logpdf(tllprior, tll‚Çö)
        if method === :projection
            ll‚Çö += dist_ll(dist, ages, tmin‚Çö, tmax‚Çö)
        elseif method === :bivariate
            ll‚Çö += dist_ll(dist, analyses, tmin‚Çö, tmax‚Çö, tll‚Çö)
        end
        ll‚Çö += prior_ll(ages, tmin‚Çö, tmax‚Çö)
        for i in eachindex(ages, ages68)
            loss = 100*max(one(T) - (ages68[i] - tll‚Çö) / (value(ages[i]) - tll‚Çö), zero(T))
            ll‚Çö += logpdf(lossprior, loss)
        end
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö - ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö - tmin) * stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö - tmax) * stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
            tll = tll‚Çö
        end
    end
    # Step through each of the N steps in the Markov chain
    @inbounds for i in eachindex(tmindist, tlldist)
        tmin‚Çö, tmax‚Çö, tll‚Çö = tmin, tmax, tll
        # Adjust upper or lower bounds, or Pb-loss time
        r = rand()
        if r < 0.35
            tmin‚Çö += tminstep * randn()
        elseif r < 0.70
            tmax‚Çö += tmaxstep * randn()
        else
            tll‚Çö += tllstep * randn()
        end
        # Flip bounds if reversed
        (tmin‚Çö > tmax‚Çö) && ((tmin‚Çö, tmax‚Çö) = (tmax‚Çö, tmin‚Çö))

        # Calculate log likelihood for new proposal
        @. ages = upperintercept(tll‚Çö, ellipses)
        ll‚Çö = logpdf(tllprior, tll‚Çö)
        if method === :projection
            ll‚Çö += dist_ll(dist, ages, tmin‚Çö, tmax‚Çö)
        elseif method === :bivariate
            ll‚Çö += dist_ll(dist, analyses, tmin‚Çö, tmax‚Çö, tll‚Çö)
        end
        ll‚Çö += prior_ll(ages, tmin‚Çö, tmax‚Çö)
        for i in eachindex(ages, ages68)
            loss = 100*max(one(T) - (ages68[i] - tll‚Çö) / (value(ages[i]) - tll‚Çö), zero(T))
            ll‚Çö += logpdf(lossprior, loss)
        end
        # Decide to accept or reject the proposal
        if log(rand()) < (ll‚Çö - ll)
            if tmin‚Çö != tmin
                tminstep = abs(tmin‚Çö - tmin) * stepfactor
            end
            if tmax‚Çö != tmax
                tmaxstep = abs(tmax‚Çö - tmax) * stepfactor
            end

            ll = ll‚Çö
            tmin = tmin‚Çö
            tmax = tmax‚Çö
            tll = tll‚Çö
            acceptancedist[i]=true
        end
        tmindist[i] = tmin
        tmaxdist[i] = tmax
        tlldist[i] = tll
        lldist[i] = ll
    end
    return tmindist, tmaxdist, tlldist, lldist, acceptancedist
end
